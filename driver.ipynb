{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        linear_1_in_features,\n",
    "        linear_1_out_features,\n",
    "        f_function,\n",
    "        linear_2_in_features,\n",
    "        linear_2_out_features,\n",
    "        g_function\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            linear_1_in_features: the in features of first linear layer\n",
    "            linear_1_out_features: the out features of first linear layer\n",
    "            linear_2_in_features: the in features of second linear layer\n",
    "            linear_2_out_features: the out features of second linear layer\n",
    "            f_function: string for the f function: relu | sigmoid | identity\n",
    "            g_function: string for the g function: relu | sigmoid | identity\n",
    "        \"\"\"\n",
    "        self.f_function = f_function\n",
    "        self.g_function = g_function\n",
    "\n",
    "        self.activations = {'relu': nn.ReLU(), 'sigmoid': nn.Sigmoid(),\n",
    "                            'identity': nn.Identity()}\n",
    "\n",
    "        try:\n",
    "            self.activations[self.f_function]\n",
    "        except KeyError:\n",
    "            print('The function f is not valid. Defaulting to identity.')\n",
    "            self.f_function = 'identity'\n",
    "\n",
    "        try:\n",
    "            self.activations[self.g_function]\n",
    "        except KeyError:\n",
    "            print('The function g is not valid. Defaulting to identity.')\n",
    "            self.g_function = 'identity'\n",
    "\n",
    "        self.parameters = dict(\n",
    "            W1=torch.randn(linear_1_out_features, linear_1_in_features),\n",
    "            b1=torch.randn(linear_1_out_features),\n",
    "            W2=torch.randn(linear_2_out_features, linear_2_in_features),\n",
    "            b2=torch.randn(linear_2_out_features),\n",
    "        )\n",
    "        self.grads = dict(\n",
    "            dJdW1=torch.zeros(linear_1_out_features, linear_1_in_features),\n",
    "            dJdb1=torch.zeros(linear_1_out_features),\n",
    "            dJdW2=torch.zeros(linear_2_out_features, linear_2_in_features),\n",
    "            dJdb2=torch.zeros(linear_2_out_features),\n",
    "        )\n",
    "\n",
    "        # put all the cache value you need in self.cache\n",
    "        self.cache = dict()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tensor shape (batch_size, linear_1_in_features)\n",
    "        \"\"\"\n",
    "        self.cache['x'] = x\n",
    "        z1 = torch.matmul(x, self.parameters['W1'].t()) + self.parameters['b1']\n",
    "        self.cache['z1'] = z1\n",
    "\n",
    "        z2 = self.activations[self.f_function](z1)\n",
    "        self.cache['z2'] = z2\n",
    "\n",
    "        z3 = torch.matmul(z2, self.parameters['W2'].t())+self.parameters['b2']\n",
    "        self.cache['z3'] = z3\n",
    "\n",
    "        y_hat = self.activations[self.g_function](z3)\n",
    "        self.cache['y_hat'] = y_hat\n",
    "\n",
    "        return self.cache['y_hat']\n",
    "\n",
    "    def grad_backprop_helper(self, func, input_):\n",
    "        sigma = torch.nn.Sigmoid()\n",
    "        z = sigma(input_)\n",
    "        grad_mappings = {'relu': torch.ones(input_.size()) * (input_ > 0),\n",
    "                         'sigmoid': torch.mul(z, 1-z),\n",
    "                         'identity': torch.ones(input_.size())}\n",
    "        return grad_mappings[func]\n",
    "\n",
    "    def backward(self, dJdy_hat):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)\n",
    "        \"\"\"\n",
    "        # TODO: Implement the backward function\n",
    "\n",
    "        # At final layer\n",
    "        batch_size = dJdy_hat.shape[0]\n",
    "        dydz3 = self.grad_backprop_helper(self.g_function, self.cache['z3'])\n",
    "        dJdz3 = torch.mul(dJdy_hat, dydz3)\n",
    "        self.grads['dJdb2'] = torch.matmul(dJdz3.t(), torch.ones(batch_size))\n",
    "        self.grads['dJdW2'] = torch.matmul(dJdz3.t(), self.cache['z2'])\n",
    "\n",
    "        # First linear layer\n",
    "        dz3dz2 = self.parameters['W2']\n",
    "        dz2dz1 = self.grad_backprop_helper(self.f_function, self.cache['z1'])\n",
    "        dJdz1 = dz2dz1 * (dJdz3 @ dz3dz2)\n",
    "\n",
    "        # First linear layer grads\n",
    "        self.grads['dJdb1'] = torch.matmul(dJdz1.t(), torch.ones(batch_size))\n",
    "        self.grads['dJdW1'] = torch.matmul(dJdz1.t(), self.cache['x'])\n",
    "\n",
    "    def clear_grad_and_cache(self):\n",
    "        for grad in self.grads:\n",
    "            self.grads[grad].zero_()\n",
    "        self.cache = dict()\n",
    "\n",
    "\n",
    "def mse_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y: the label tensor (batch_size, linear_2_out_features)\n",
    "        y_hat: the prediction tensor (batch_size, linear_2_out_features)\n",
    "    Return:\n",
    "        J: scalar of loss\n",
    "        dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)\n",
    "    \"\"\"\n",
    "    # TODO: Implement the mse loss\n",
    "    loss = torch.pow((y_hat-y), 2).mean()\n",
    "    dJdy_hat = 2*(y_hat - y)/(y.shape[0]*y.shape[1])\n",
    "\n",
    "    return loss, dJdy_hat\n",
    "\n",
    "\n",
    "def bce_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y_hat: the prediction tensor\n",
    "        y: the label tensor\n",
    "\n",
    "    Return:\n",
    "        loss: scalar of loss\n",
    "        dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)\n",
    "    \"\"\"\n",
    "    # TODO: Implement the bce loss\n",
    "    loss = - (y * torch.clamp(torch.log(y_hat), min=-100) + (1-y)\n",
    "              * torch.clamp(torch.log(1-y_hat), min=-100)).mean()\n",
    "    dJdy_hat = (- y/y_hat + (1-y)/(1-y_hat))/(y.shape[0]*y.shape[1])\n",
    "\n",
    "    return loss, dJdy_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from mlp import MLP, mse_loss, bce_loss\n",
    "\n",
    "net = MLP(\n",
    "    linear_1_in_features=2,\n",
    "    linear_1_out_features=41,\n",
    "    f_function='relu',\n",
    "    linear_2_in_features=41,\n",
    "    linear_2_out_features=11,\n",
    "    g_function='identity'\n",
    ")\n",
    "x = torch.randn(10, 2)\n",
    "y = torch.randn(10, 11)\n",
    "\n",
    "net.clear_grad_and_cache()\n",
    "y_hat = net.forward(x)\n",
    "J, dJdy_hat = mse_loss(y, y_hat)\n",
    "net.backward(dJdy_hat)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# compare the result with autograd\n",
    "net_autograd = nn.Sequential(\n",
    "    OrderedDict([\n",
    "        ('linear1', nn.Linear(2, 40)),\n",
    "        ('relu', nn.ReLU()),\n",
    "        ('linear2', nn.Linear(40, 10)),\n",
    "    ])\n",
    ")\n",
    "net_autograd.linear1.weight.data = net.parameters['W1']\n",
    "net_autograd.linear1.bias.data = net.parameters['b1']\n",
    "net_autograd.linear2.weight.data = net.parameters['W2']\n",
    "net_autograd.linear2.bias.data = net.parameters['b2']\n",
    "\n",
    "y_hat_autograd = net_autograd(x)\n",
    "\n",
    "J_autograd = F.mse_loss(y_hat_autograd, y)\n",
    "\n",
    "net_autograd.zero_grad()\n",
    "J_autograd.backward()\n",
    "\n",
    "print((net_autograd.linear1.weight.grad.data -\n",
    "      net.grads['dJdW1']).norm() < 1e-3)\n",
    "print((net_autograd.linear1.bias.grad.data - net.grads['dJdb1']).norm() < 1e-3)\n",
    "print((net_autograd.linear2.weight.grad.data -\n",
    "      net.grads['dJdW2']).norm() < 1e-3)\n",
    "print((net_autograd.linear2.bias.grad.data - net.grads['dJdb2']).norm() < 1e-3)\n",
    "# ------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from mlp import MLP, mse_loss, bce_loss\n",
    "\n",
    "net = MLP(\n",
    "    linear_1_in_features=2,\n",
    "    linear_1_out_features=13,\n",
    "    f_function='relu',\n",
    "    linear_2_in_features=13,\n",
    "    linear_2_out_features=22,\n",
    "    g_function='relu'\n",
    ")\n",
    "x = torch.randn(10, 2)\n",
    "y = torch.randn(10, 22)\n",
    "\n",
    "net.clear_grad_and_cache()\n",
    "y_hat = net.forward(x)\n",
    "J, dJdy_hat = mse_loss(y, y_hat)\n",
    "net.backward(dJdy_hat)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# compare the result with autograd\n",
    "net_autograd = nn.Sequential(\n",
    "    OrderedDict([\n",
    "        ('linear1', nn.Linear(2, 13)),\n",
    "        ('relu1', nn.ReLU()),\n",
    "        ('linear2', nn.Linear(13, 22)),\n",
    "        ('relu2', nn.ReLU()),\n",
    "    ])\n",
    ")\n",
    "net_autograd.linear1.weight.data = net.parameters['W1']\n",
    "net_autograd.linear1.bias.data = net.parameters['b1']\n",
    "net_autograd.linear2.weight.data = net.parameters['W2']\n",
    "net_autograd.linear2.bias.data = net.parameters['b2']\n",
    "\n",
    "y_hat_autograd = net_autograd(x)\n",
    "\n",
    "J_autograd = F.mse_loss(y_hat_autograd, y)\n",
    "\n",
    "net_autograd.zero_grad()\n",
    "J_autograd.backward()\n",
    "\n",
    "print((net_autograd.linear1.weight.grad.data -\n",
    "      net.grads['dJdW1']).norm() < 1e-3)\n",
    "print((net_autograd.linear1.bias.grad.data - net.grads['dJdb1']).norm() < 1e-3)\n",
    "print((net_autograd.linear2.weight.grad.data -\n",
    "      net.grads['dJdW2']).norm() < 1e-3)\n",
    "print((net_autograd.linear2.bias.grad.data - net.grads['dJdb2']).norm() < 1e-3)\n",
    "# ------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from mlp import MLP, mse_loss, bce_loss\n",
    "\n",
    "net = MLP(\n",
    "    linear_1_in_features=2,\n",
    "    linear_1_out_features=30,\n",
    "    f_function='sigmoid',\n",
    "    linear_2_in_features=30,\n",
    "    linear_2_out_features=5,\n",
    "    g_function='sigmoid'\n",
    ")\n",
    "x = torch.randn(10, 2)\n",
    "y = (torch.randn(10, 5) < 0.5) * 1.0\n",
    "\n",
    "net.clear_grad_and_cache()\n",
    "y_hat = net.forward(x)\n",
    "J, dJdy_hat = bce_loss(y, y_hat)\n",
    "net.backward(dJdy_hat)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# compare the result with autograd\n",
    "net_autograd = nn.Sequential(\n",
    "    OrderedDict([\n",
    "        ('linear1', nn.Linear(2, 30)),\n",
    "        ('sigmoid1', nn.Sigmoid()),\n",
    "        ('linear2', nn.Linear(30, 5)),\n",
    "        ('sigmoid2', nn.Sigmoid()),\n",
    "    ])\n",
    ")\n",
    "net_autograd.linear1.weight.data = net.parameters['W1']\n",
    "net_autograd.linear1.bias.data = net.parameters['b1']\n",
    "net_autograd.linear2.weight.data = net.parameters['W2']\n",
    "net_autograd.linear2.bias.data = net.parameters['b2']\n",
    "\n",
    "y_hat_autograd = net_autograd(x)\n",
    "\n",
    "J_autograd = torch.nn.BCELoss()(y_hat_autograd, y)\n",
    "\n",
    "net_autograd.zero_grad()\n",
    "J_autograd.backward()\n",
    "\n",
    "print((net_autograd.linear1.weight.grad.data -\n",
    "      net.grads['dJdW1']).norm() < 1e-3)\n",
    "print((net_autograd.linear1.bias.grad.data - net.grads['dJdb1']).norm() < 1e-3)\n",
    "print((net_autograd.linear2.weight.grad.data -\n",
    "      net.grads['dJdW2']).norm() < 1e-3)\n",
    "print((net_autograd.linear2.bias.grad.data - net.grads['dJdb2']).norm() < 1e-3)\n",
    "# ------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (30): ReLU(inplace=True)\n",
      "    (31): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (32): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (33): ReLU(inplace=True)\n",
      "    (34): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "ANSWER_FOR_LABEL_0: 1.0\n",
      "ANSWER_FOR_LABEL_12: 1.0\n",
      "ANSWER_FOR_LABEL_954: 1.0\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models import VGG13_BN_Weights, vgg13_bn\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "def save_img(image, path):\n",
    "    # Push to CPU, convert from (1, 3, H, W) into (H, W, 3)\n",
    "    image = image[0].permute(1, 2, 0)\n",
    "    image = image.clamp(min=0, max=1)\n",
    "    image = (image * 255).cpu().detach().numpy().astype(np.uint8)\n",
    "    # opencv expects BGR (and not RGB) format\n",
    "    cv.imwrite(path, image[:, :, ::-1])\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = vgg13_bn(VGG13_BN_Weights.IMAGENET1K_V1).to(DEVICE)\n",
    "    print(model)\n",
    "    for label in [0, 12, 954]:\n",
    "        image = torch.randn(1, 224, 224, 3).to(DEVICE)\n",
    "        image = (image * 8 + 128) / 255  # background color = 128,128,128\n",
    "        image = image.permute(0, 3, 1, 2)\n",
    "        image.requires_grad_()\n",
    "        image = gradient_descent(\n",
    "            image, model, lambda tensor: tensor[0, label].mean(),)\n",
    "        save_img(image, f\"./img_{label}.jpg\")\n",
    "        out = model(image)\n",
    "        print(f\"ANSWER_FOR_LABEL_{label}: {out.softmax(1)[0, label].item()}\")\n",
    "\n",
    "\n",
    "# DO NOT CHANGE ANY OTHER FUNCTIONS ABOVE THIS LINE FOR THE FINAL SUBMISSION\n",
    "\n",
    "\n",
    "def normalize_and_jitter(img, step=32):\n",
    "    # You should use this as data augmentation and normalization,\n",
    "    # convnets expect values to be mean 0 and std 1\n",
    "    dx, dy = np.random.randint(-step, step - 1, 2)\n",
    "    return transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)(\n",
    "        img.roll(dx, -1).roll(dy, -2)\n",
    "    )\n",
    "\n",
    "\n",
    "def gradient_descent(input, model, loss, iterations=256):\n",
    "    input = normalize_and_jitter(input)\n",
    "    input = torch.nn.Parameter(input)\n",
    "    lr = 0.01\n",
    "    for _ in range(iterations):\n",
    "        x = model(input)\n",
    "        l = loss(x)\n",
    "\n",
    "        l.backward()\n",
    "\n",
    "        input.data = input.data + lr * input.grad.data\n",
    "        input.grad.data.zero_()\n",
    "\n",
    "    return input\n",
    "\n",
    "\n",
    "def forward_and_return_activation(model, input, module):\n",
    "    \"\"\"\n",
    "    This function is for the extra credit. You may safely ignore it.\n",
    "    Given a module in the middle of the model (like `model.features[20]`),\n",
    "    it will return the intermediate activations.\n",
    "    Try setting the modeul to `model.features[20]` and the loss to `tensor[0, ind].mean()`\n",
    "    to see what intermediate activations activate on.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    def hook(model, input, output):\n",
    "        features.append(output)\n",
    "\n",
    "    handle = module.register_forward_hook(hook)\n",
    "    model(input)\n",
    "    handle.remove()\n",
    "\n",
    "    return features[0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "850e86733fd11e39368ad0f4d5057a184573ae93c3821caecd584c5b56b00dd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
