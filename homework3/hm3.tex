%This is my super simple Real Analysis Homework template

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath,bm}
\usepackage{minted}
\usepackage[]{tcolorbox}
\usepackage[]{amsthm} %lets us use \begin{proof}
\usepackage[]{amssymb} %gives us the character \varnothing
\usepackage{mathtools}
\DeclareMathOperator{\sech}{sech}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\title{Homework 3}
\author{Guilherme Albertini}
\date\today
%This information doesn't actually show up on your document unless you use the maketitle command below

\begin{document}
\maketitle %This command prints the title based on information entered above

%Section and subsection automatically number unless you put the asterisk next to them.
\section*{Theory}
% References: 
% https://distill.pub/2019/computing-receptive-fields/
% https://arxiv.org/abs/1603.07285
\subsection*{Problem 1.1: Energy Based Models Intuition}
\begin{enumerate}
	\item How do energy-based models allow for modeling situations where
	      the
	      mapping from input $x_i$ to output $y_i$ is not 1-to-1, but
	      1-to-many?
	      % https://deepai.org/machine-learning-glossary-and-terms/energy-based-models
	      \begin{tcolorbox}
		      We are mapping pairs $(x,y)$ to a scalar energy value
		      and find that the most likely values of $y$ have a low
		      $F(x,y)$. We then
		      observe that, for each $x$, we can have several different
		      values $y$ that have
		      this low energy.
	      \end{tcolorbox}
	      \begin{tcolorbox}

		      Note a good definition from DeepAI: ``Energy-Based
		      Models (EBMs) discover data dependencies by applying a
		      measure of
		      compatibility (scalar energy) to each configuration of
		      the variables. For a model to make a prediction or
		      decision (inference)
		      it needs to set the value of observed variables to 1
		      and finding values of the remaining variables that
		      minimize that “energy”
		      level.
		      In the same way, machine learning consists of
		      discovering an energy function that assigns low energies
		      to the correct values
		      of the remaining variables,
		      and higher energies to the incorrect values. A
		      so-called “loss functional,” that is minimized during
		      training, is used to
		      measure the quality of the energy functions. Within this
		      framework, there are
		      many energy functions and loss functionals allows
		      available to design different
		      probabilistic and non-probabilistic statistical
		      models.''\\
		      Words from Alf: ``We would like the energy function to
		      be smooth and differentiable so that we can use it to
		      perform the
		      gradient-based method for inference. In order to perform
		      inference, we search
		      this function using gradient descent to find compatible
		      y's. There are many
		      alternate methods to gradient methods to obtain the
		      minimum.''
	      \end{tcolorbox}

	\item How do energy-based models differ from models that output
	      probabilities?
	      \begin{tcolorbox}
		      As is the key to their flexibility, we need not concern
		      ourselves
		      with normalization as EBMs output an unnormalized scalar
		      (score) of $F(x,y)$ as
		      opposed to conditional probabilities (i.e
		      $\mathbb{P}(y|x)$ would later require
		      an estimate of normalization).
	      \end{tcolorbox}

	\item How can you use energy function $F_{W}(x, y)$ to calculate a
	      probability $\mathbb{P}(y|x)$?
	      \begin{tcolorbox}
		      We can view energies as unnormalised negative log
		      probabilities, and use Gibbs-Boltzmann distribution to
		      convert from energy to
		      probability (with normalization and calibrated $\beta$):
		      \begin{flalign*}
			      \mathbb{P}(y|x)=\frac{\exp(-\beta
				      F(x,y))}{\int_{y'}\exp(-\beta F(x,y'))}
		      \end{flalign*}
		      Note: $\beta$ is postive constant and larger values
		      produce models with more variance whereas smoother ones
		      are produced with
		      smaller values.
	      \end{tcolorbox}
	\item What are the roles of the loss function and energy function?
	      \begin{tcolorbox}
		      The energy function is a measure of incompatibility
		      between variables (for us, usually the input $x$ and
		      output $y$) whereas the
		      loss function is used to mold the energy function (we
		      minimize loss to end up
		      with a well-behaved energy function). Note that the cost
		      is how far prediction
		      $\hat{y}$ is from target $y$. As Yann mentions: A loss
		      functional, minimized
		      during learning, is used to measure the quality of the
		      available energy
		      functions. A distinction should be made between the
		      energy function, which is
		      minimized by the inference process, and the loss
		      functional, which is minimized
		      by the learning process.
	      \end{tcolorbox}

	\item What problems can be caused by using only positive examples for
	      energy (pushing down energy of correct inputs only)? How can it
	      be avoided?
	      \begin{tcolorbox}
		      We may get a case of having energy be 0 everywhere,
		      which is a valid minimization of the (flat) energy
		      surface under this constraint. As this flat model can
		      reach every location of
		      the space, the distance between any two points (such as
		      the length of the
		      latent vector spanning the embedded model manifold's
		      reconstructed
		      $\tilde{y}=Wz$ to observed target $y$ from the data
		      manifold) is 0, hence at
		      the minimum energy by default. To avoid this degenerate
		      case, we can augment $y
			      = [1, y]^{T}$ to give an additional degree of
		      freedom to dictionary $W=[1, W]^{T}$, so
		      that we can now intersect any point in the 2D space but
		      only at those points
		      located at the specific height (here, 1) that gives the
		      minimum energy near 0.

	      \end{tcolorbox}
	\item Briefly explain the three methods that can be used to shape the
	      energy function.
		  % Summarizing from : https://atcold.github.io/pytorch-Deep-Learning/en/week08/08-2/
	      \begin{tcolorbox}
		      Regularization Methods: if the latent variable z is too
		      expressive power in producing the final prediction
		      $\tilde{y}$ then every true
		      output $y$ will be a perfect reconstruction from input
		      $x$ at the optimized
		      latent $\check{z}$. We can then limit the volume of space of $z$ (say, with L1 loss to promote sparsity) and therby reduce the regions of $y$ with low energy, preventing the case of getting energy 0 everywhere.\\
			  Contrastive Methods: Push down the energy of training data points, $F(X_i,Y_i)$, while pushing up energy on everywhere else, $F(X_i,Y')$.\\
			  Architectural Methods: the manifold is of lower dimension than the ambient space so the data cannot be reconstructred perfectly. 
	      \end{tcolorbox}
	\item Provide an example of a loss function that uses negative
	      examples.The format should be as follows `example(x, y,W) = FW
	      (x, y).
\end{enumerate}

\end{document}