%This is my super simple Real Analysis Homework template

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath,bm}
\usepackage{minted}
\usepackage[]{tcolorbox}
\usepackage[]{amsthm} %lets us use \begin{proof}
\usepackage[]{amssymb} %gives us the character \varnothing
\usepackage{mathtools}
\DeclareMathOperator{\sech}{sech}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\title{Homework 2}
\author{Guilherme Albertini}
\date\today
%This information doesn't actually show up on your document unless you use the maketitle command below

\begin{document}
\maketitle %This command prints the title based on information entered above

%Section and subsection automatically number unless you put the asterisk next to them.
\section*{Theory}
\subsection*{Problem 1.1: Convolutional Neural Networks}
\begin{enumerate}
  \item Given an input image of dimension $21 \times 12$, what will be output
        dimension after applying a convolution with $4 \times 5$ kernel, stride
        of 4,
        and no padding?
        \begin{tcolorbox}
          \begin{flalign*}
            5 \times 2
          \end{flalign*}
        \end{tcolorbox}

  \item Given an input of dimension $C \times H \times W$ what will be the
        dimension of the output of a convolutional layer with kernel of size $K
          \times
          K$, padding P, stride S, dilation D, and F filters. Assume that H
        $\geq$ K, W
        $\geq$ K.
        \begin{tcolorbox}
          Define Padding along height on top	  $P_{H1}$\\

          Define Padding along height on bottom $P_{H2}$\\

          Define Padding along width on left	  $P_{W1}$\\

          Define Padding along width on right	$P _{W2}$\\

          Define Kernel width		 $K_{H}$\\

          Define Kernel height		  $K_{W}$\\

          Define Stride horizontal		  $S_{W}$\\

          Define Stride vertical		  $S_{H}$\\

          Define Batch Count			  $B$\\

          \begin{flalign*}
                   & \text{Effect of adding padding and applying kernel to
              dimensions:}
            \\
            H_P    & = P_{H1} + P_{H2} +
            H                                                              \\
            W_P    & = P_{W1} + P_{W2} +
            W                                                              \\
            H_{PK} & = H_1 - [D_H(K_H - 1)+1]                              \\
                   & =P_{H1} + P_{H2} + H - [D_H(K_H - 1)+1]
            \\
            W_{PK} & = W_1 - [D_W(K_W - 1)+1]                              \\
                   & =P_{W1} + P_{W2} + W - [D_W(K_W - 1)+1]
            \\
          \end{flalign*}
        \end{tcolorbox}
        \begin{tcolorbox}
          \begin{flalign*}
                    & \text{Considering stride to
            dimensions:}                                              \\
            H_{PKS} & = \floor*{\frac{H_P -
            [D_H(K_H - 1)+1]+S_H}{S_H}}
            \\
                    & =\floor*{\frac{P_{H1} + P_{H2} + H - [D_H(K_H -
            1)+1]}{S_H}}+1                                            \\
            W_{PKS} & = \floor*{\frac{W_P -
            [D_W(K_W - 1)+1]+S_W}{S_W}}
            \\
                    & =\floor*{\frac{P_{W1} + P_{W2} + W - [D_W(K_W -
            1)+1]}{S_W}}+1
          \end{flalign*}
        \end{tcolorbox}
        \begin{tcolorbox}
          We can make simplifcations that I think are implied here:
          \begin{flalign*}
            S & = S_W = S_H                         \\
            D & = D_W = D_H                         \\
            K & = K_W = K_H                         \\
            B & = 1                                 \\
            P & = P_{W1} + P_{W2} = P_{H1} + P_{H2} \\
          \end{flalign*}
          \begin{flalign*}
                     & \text{Thus the output dimension is: }             \\
            F \times & \left( \floor*{\frac{2P + H - [D(K - 1)+1]}{S}}+1
            \right)                                                      \\
            \times   & \left(\floor*{\frac{2P + W - [D(K - 1)+1]}{S}}+1
            \right)
          \end{flalign*}
        \end{tcolorbox}
  \item Let's consider an input $x[n] \in \mathbb{R}^5$, with $1 \leq n \leq
          7$, e.g. it is a length 7 sequence with 5 channels. We consider the
        convolutional layer $f_W$ with one filter, with kernel size 3, stride of 2, no
        dilation, and no padding. The only
        parameters of the convolutional layer is the weight $W, W \in \mathbb{R}^{1
            \times 5 \times 3}$ and there is no bias and no non-linearity.
        \begin{enumerate}
          \item What is the dimension of the output $f_W(x)$? Provide an
                expression for the value of elements of the convolutional layer output
                $f_W(x)$.
                Example answer format here and in the following sub-problems: $f_W(x)
                  \in \mathbb{R}^{42 \times 42 \times 42}, f_W(x)[i, j,k] = 42.$
                \begin{tcolorbox}
                  \begin{flalign*}
                    f_W(x)    & \in \mathbb{R}^2                                     \\
                    f_W(x)[r] & = \sum_{k=1}^{5}\sum_{k=1}^{3} x[i+2(r-1)][k]W_{k,i}
                  \end{flalign*}
                \end{tcolorbox}\
          \item What is the dimension of $\frac{\partial f_W(x)}{\partial W}$?
                What are its values?
                \begin{tcolorbox}
                  \begin{flalign*}
                    \frac{\partial f_W(x)}{\partial W}          & \in \mathbb{R}^{5 \times (1
                    \times 5 \times 3)}                                                       \\
                    \frac{\partial f_W(x)}{\partial W}[r,c,i,k] & =x[i+2(r-1)][k]
                  \end{flalign*}
                \end{tcolorbox}
          \item What is the dimension of $\frac{\partial f_W(x)}{\partial x}$?
                What are its values?
                \begin{tcolorbox}
                  \begin{flalign*}
                    \frac{\partial f_W(x)}{\partial x}        & \in \mathbb{R}^{2 \times (2
                    \times 7)}                                                                                    \\
                    \frac{\partial f_W(x)}{\partial x}[r,k,i] & = \begin{cases}
                                                                    W_{1,k,i-2(r-1)} & \text{if } i-2(r-1) \in [1,3] \\
                                                                    0                & \text{otherwise }
                                                                  \end{cases}
                  \end{flalign*}
                \end{tcolorbox}
          \item Now, suppose you are given the gradient of the loss $\ell$ with
                respect to the output of the convolutional layer $f_W(x)$, i.e. $\frac{\partial
                    \ell}{\partial f_W(x)}$. What is the dimension of $\frac{\partial
                    \ell}{\partial W}$? Provide its expression. Explain the similarities and
                differences of this and expression in (a).
                \begin{tcolorbox}
                  \begin{flalign*}
                    \frac{\partial \ell}{\partial W}                       & \in \mathbb{R}^{a \times b
                    \times c}                                                                           \\
                    \left( \frac{\partial \ell }{\partial W}\right)[1,k,i] & = \sum_{r
                      = 1}^{23232}\left(\frac{\partial \ell}{\partial f_W(x)}\right)[r]x[i+2(r-1),k]
                  \end{flalign*}
                  The difference is that this is a dilated convolution. Both the
                  backward and forward pass of the conv. layer apply a convolution but the stride
                  dilates in the backward pass.
                \end{tcolorbox}
        \end{enumerate}
\end{enumerate}

%If you want centered math on its own line, you can use a slash and square
%bracket.\\
%\[
%  \left \{
%  \sum\limits_{k=1}^\infty l(I_k):A\subseteq \bigcup_{k=1}^\infty \{I_k\}
%  \right \}
%\]
%The left and right commands make the brackets get as big as we need them to
%be.

\clearpage %Gives us a page break before the next section. Optional.
\subsection*{Problem 1.2: Recurrent Neural Networks}

In this section consider simple recurrent neural network defined by:
\begin{flalign}
  c[t] &=\sigma(W_{c}x[t]+W_h[h][t-1])\\
  z[t]&=c[t]\odot h[t-1] + (1-c[t]) \odot W_{x}x[t]
\end{flalign} 
here $\sigma$ is element-wise sigmoid, $x[t]\in \mathbb{R}^n, h[t]
  \in \mathbb{R}^m, W_c \in \mathbb{R}^{m \times n}, W_h \in \mathbb{R}^{m \times
    m}, W_x \in \mathbb{R}^{m \times n}$ and $\odot$ is a Hadamard product, $h[0]
  \coloneqq 0.$
\begin{enumerate}
  \item Draw a diagram for this RNN.
  \item What is the dimension of $c[t]$?
  \item Suppose that we run the RNN to get a sequence of $h[t]$ for t from 1 to
        K. Assumging we know the derivative $\frac{\partial \ell}{\partial h[t]}$,
        provide the dimension of an expression for values of $\frac{\partial
            \ell}{\partial W_x}$. What are the similarities and differences between
        backward and forward pass of RNN?
  \item  Can this network be subject to vanishing or exploding gradients?
\end{enumerate}

\subsection*{Problem 1.3: AttentionRNN(2)}

Now define AttentionRNN(2) as:
\begin{flalign}
  q_0[t],q_1[t],q_2[t] & = Q_0x[t],Q_1h[t-1],Q_2h[t-2]               \\
  k_0[t],k_1[t],k_2[t] & = K_0x[t],K_1h[t-1],K_2h[t-2]               \\
  v_0[t],v_1[t],v_2[t] & = V_0x[t],V_1h[t-1],V_2h[t-2]               \\
  w_i[t]               & = q_i[t]^{T}k_i[t]                          \\
  a[t]                 & = \text{softargmax}([w_0[t],w_1[t],w_2[t]]) \\
  h[t]                 & = \sum_{i=0}^{2}a_i[t]v_i[t]
\end{flalign}
where $x_i[t],h[t] \in \mathbb{R}^n$ and $Q_i, K_i, V_i \in \mathbb{R}^{n
    \times n}.$ We define $h[t] = 0$ for $t < 1$. You may safely ignore base cases
in the following.
\begin{enumerate}
  \item Draw a diagram for this RNN.
  \item What is the dimension of $a[t]$?
  \item Extend this to AttentionRNN(k), a netowrk that uses the last k state
        vectors h. Write out a system of equations that defines it.
  \item Modify the above netork to produce AttentionRNN($\infty$), a network
        that uses every past state vector. Write out a system of equations that defines
        it.
  \item Suppose the loss $\ell$ is computed, and we know the derivative
        $\frac{\partial \ell}{\partial h[i]}$ for all $i \geq t$. Write down expression
        for $\frac{\partial h[t]}{\partial h[t-1]}$ for AttentionRNN(2).
  \item Suppose we know $\frac{\partial h[t]}{\partial h[T]}$ and
        $\frac{\partial \ell}{\partial h[t]} \forall t>T$. Write down expression for
        $\frac{\partial \ell}{\partial h[T]}$ for AttentionRNN(k).
\end{enumerate}

\subsection*{Problem 1.4: Debugging Loss Curves}
%
\begin{enumerate}
  \item What causes the spikes on the left?
  \item How can they be higher than the initial value of the loss?
  \item What are some ways to fix them?
  \item Explain why the loss and accuracy are at these set values before
        training starts. You may need to check the task definition in the notebook,
\end{enumerate}

\end{document}