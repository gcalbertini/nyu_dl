{"cells":[{"cell_type":"markdown","metadata":{"id":"PtcBjMq7YV3f"},"source":["# Homework 2 - Recurrent Neural Networks\n"]},{"cell_type":"markdown","metadata":{"id":"Rn-cOk1iZTtR"},"source":["In this part of the homework we are going to work with Recurrent Neural Networks, in particular GRU. One of the greatest things that Recurrent Neural Networks can do when working with sequences is retaining data from several timesteps in the past. We are going to explore that property by constructing an 'echo' Recurrent Neural Network.\n","\n","The goal here is to make a model that given a sequence of letters or digits will output that same sequence, but with a certain delay. Let's say the input is a string 'abacaba', we want the model to not output anything for 3 steps (delay length), and then output the original string step by step, except the last 3 characters. So, target output is then 'XXXabac', where 'X' is empty output.\n","\n","This is similar to [this notebook](https://github.com/Atcold/pytorch-Deep-Learning/blob/master/09-echo_data.ipynb) (which you should refer to when doing this assignment), except we're working not with a binary string, but with a sequence of integers between 0 and some N. In our case N is 26, which is the number of letters in the alphabet.\n"]},{"cell_type":"markdown","metadata":{"id":"npLlE973as6x"},"source":["## Dataset\n","\n","Let's implement the dataset. In our case, the data is basically infinite, as we can always generate more examples on the fly, so there's no need to load it from disk.\n"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"mkEEMyvzIMRx"},"outputs":[],"source":["import random\n","import string\n","\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","import torch.optim as optim\n","from tqdm.notebook import tqdm\n","\n","\n","\n","# Max value of the generated integer. 26 is chosen because it's\n","# the number of letters in English alphabet.\n","N = 26\n","\n","\n","def idx_to_onehot(x, k=N+1):\n","    \"\"\" Converts the generated integers to one-hot vectors \"\"\"\n","    ones = torch.sparse.torch.eye(k)\n","    shape = x.shape\n","    res = ones.index_select(0, x.view(-1).type(torch.int64))\n","    return res.view(*shape, res.shape[-1])\n","\n","\n","class EchoDataset(torch.utils.data.IterableDataset):\n","\n","    def __init__(self, delay=4, seq_length=15, size=1000):\n","        self.delay = delay\n","        self.seq_length = seq_length\n","        self.size = size\n","\n","    def __len__(self):\n","        return self.size\n","\n","    def __iter__(self):\n","        \"\"\" Iterable dataset doesn't have to implement __getitem__.\n","            Instead, we only need to implement __iter__ to return\n","            an iterator (or generator).\n","        \"\"\"\n","        for _ in range(self.size):\n","            seq = torch.tensor([random.choice(range(1, N + 1))\n","                               for i in range(self.seq_length)], dtype=torch.int64)\n","            result = torch.cat(\n","                (torch.zeros(self.delay), seq[:self.seq_length - self.delay])).type(torch.int64)\n","            yield seq, result\n","\n","\n","DELAY = 4\n","DATASET_SIZE = 1000#200000\n","ds = EchoDataset(delay=DELAY, size=DATASET_SIZE)\n"]},{"cell_type":"markdown","metadata":{"id":"nNrZqYURcKSl"},"source":["## Model\n","\n","Now, we want to implement the model. For our purposes, we want to use GRU. The architecture consists of GRU and a decoder. Decoder is responsible for decoding the GRU hidden state to yield a predicting for the next output. The parts you are responsible for filling with your code are marked with `TODO`.\n"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"nigN_o4Mb9Nx"},"outputs":[{"name":"stdout","output_type":"stream","text":["device: cuda\n"]}],"source":["input_size = N + 1\n","sequence_len = N + 1\n","num_classes = sequence_len\n","num_layers = 2 #stck em up\n","hidden_size = 64\n","import numpy as np\n","\n","# making sure we are using GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"device:\", device)\n","\n","\n","class GRUMemory(torch.nn.Module):\n","\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super().__init__()\n","        # TODO: initialize your submodules\n","\n","        self.num_layers = num_layers\n","        self.hidden_size = hidden_size\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first = True)\n","        # x is [batch x seq x input]\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        # inputs: x - input tensor of shape (batch_size, seq_length, N+1)\n","        # returns:\n","        # logits (scores for softmax) of shape (batch size, seq_length, N + 1)\n","        # TODO implement forward pass\n","\n","        # Hidden state init for first input to 0; forward propagation by passing in the input and hidden state into the model\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n","        out, _ = self.gru(x, h0) #dont need hidden state\n","        # out: tensor of shape (batch_size, seq_length, hidden_size)\n","        # out (z, N+1, hidden size)\n","\n","        # Decode the hidden state of the last time step\n","        out = out[:, -1, :]\n","\n","        out = self.fc(self.relu(out))\n","\n","        return out\n","\n","    @torch.no_grad()\n","    def test_run(self, s):\n","        # This function accepts one string s containing lowercase characters a-z.\n","        # You need to map those characters to one-hot encodings,\n","        # then get the result from your network, and then convert the output\n","        # back to a string of the same length, with 0 mapped to ' ',\n","        # and 1-26 mapped to a-z.\n","\n","        # Use ASCII tricks: https://www.asciitable.com/ where a is 97 to z being 122 and a space is 32\n","        alpha_to_num = torch.from_numpy(np.array([ord(i)-96 if ord(i) >= 97 else ord(i) - 32 for i in list(s)]))\n","        dis_fire = idx_to_onehot(alpha_to_num).unsqueeze(0).to(device)\n","\n","        logits = self.forward(dis_fire)\n","        predict = torch.argmax(logits, dim = 2)\n","        return [chr(i + 96) if i >= 1 else chr(i+32) for i in predict[0,:]]\n","\n","model = GRUMemory(input_size, hidden_size, num_layers, num_classes).to(device)\n"]},{"cell_type":"markdown","metadata":{"id":"A9whwmVu9OIx"},"source":["## Training\n","\n","Below you need to implement the training of the model. We give you more freedom as for the implementation. The two limitations are that it has to execute within 10 minutes, and that error rate should be below 1%.\n"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"lUZkeRnVTNzG"},"outputs":[],"source":["def test_model(model, sequence_length=15):\n","    \"\"\"\n","    This is the test function that runs 100 different strings through your model,\n","    and checks the error rate.\n","    \"\"\"\n","    total = 0\n","    correct = 0\n","    for i in range(500):\n","        s = ''.join([random.choice(string.ascii_lowercase)\n","                    for i in range(random.randint(15, 25))])\n","        result = model.test_run(s)\n","        for c1, c2 in zip(s[:-DELAY], result[DELAY:]):\n","            correct += int(c1 == c2)\n","        total += len(s) - DELAY\n","\n","    return correct / total\n"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["def train(model, optimizer, criterion, device, batch):\n","\n","\tmodel.train()\n","\tdata, target = idx_to_onehot(batch[0], k=N+1).to(device), batch[1].to(device)\n","\toutput = model(data)\n","\tcount_batch_correct = 0\n","\n","\t'''Prof Note:\n","\t\tFor loss functions like CrossEntropyLoss,\n","        # the second argument is actually expected to be a tensor of class indices rather than\n","        # one-hot encoded class labels. One approach is to take advantage of the one-hot encoding\n","        # of the target and call argmax along its second dimension to create a tensor of shape\n","        # (batch_size) containing the index of the class label that was hot for each sequence.'''\n","\tpredict = torch.argmax(output, dim = 2)\n","\ttarget = torch.argmax(target, dim = 2)\n","\tloss = criterion(output, target)\n","\n","\toptimizer.zero_grad()\n","\tloss.backward()\n","\toptimizer.step()\n","\n","\tmatches = target == predict\n","\tcount_batch_correct, tot_batch = np.sum(matches), loss.item()\n","\n","\treturn tot_batch, count_batch_correct"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["def eval(model, optimizer, criterion, device, batch):\n","\t\n","\tmodel.eval()\n","\tdata, target = idx_to_onehot(batch[0], k=N+1).to(device), batch[1].to(device)\n","\toutput = model(data)\n","\tcount_batch_correct = 0\n","\n","\t'''Prof Note:\n","\t\tFor loss functions like CrossEntropyLoss,\n","        # the second argument is actually expected to be a tensor of class indices rather than\n","        # one-hot encoded class labels. One approach is to take advantage of the one-hot encoding\n","        # of the target and call argmax along its second dimension to create a tensor of shape\n","        # (batch_size) containing the index of the class label that was hot for each sequence.'''\n","\tpredict = torch.argmax(output, dim = 2)\n","\ttarget = torch.argmax(target, dim = 2)\n","\tloss = criterion(output, target)\n","\n","\toptimizer.zero_grad()\n","\tloss.backward()\n","\toptimizer.step()\n","\n","\tmatches = target == predict\n","\tcount_batch_correct, tot_batch = np.sum(matches), loss.item()\n","\n","\treturn tot_batch, count_batch_correct"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"9lV9BscxCCAI","outputId":"fc62ff98-086b-4c2c-d9bc-06c568e48800"},"outputs":[{"name":"stdout","output_type":"stream","text":["device: cuda:0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0176912447084c3b803dee793ed8c86d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d1cb50828644bf1bdf901854f325cc3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"RuntimeError","evalue":"DataLoader worker (pid(s) 7636, 17800, 29012, 39816) exited unexpectedly","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)","File \u001b[1;32mc:\\Users\\gbert\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1164\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n","File \u001b[1;32mc:\\Users\\gbert\\anaconda3\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n","\u001b[1;31mEmpty\u001b[0m: ","\nThe above exception was the direct cause of the following exception:\n","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32mc:\\Users\\gbert\\Desktop\\nyu\\nyu_dl\\homework2\\hw2_rnn.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gbert/Desktop/nyu/nyu_dl/homework2/hw2_rnn.ipynb#X13sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m total_val \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gbert/Desktop/nyu/nyu_dl/homework2/hw2_rnn.ipynb#X13sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/gbert/Desktop/nyu/nyu_dl/homework2/hw2_rnn.ipynb#X13sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(train_dataloader, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gbert/Desktop/nyu/nyu_dl/homework2/hw2_rnn.ipynb#X13sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     tot_batch, count_batch_correct \u001b[39m=\u001b[39m train(model\u001b[39m=\u001b[39mmodel,optimizer\u001b[39m=\u001b[39moptimizer,criterion\u001b[39m=\u001b[39mcriterion,device\u001b[39m=\u001b[39mdevice,batch\u001b[39m=\u001b[39mbatch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gbert/Desktop/nyu/nyu_dl/homework2/hw2_rnn.ipynb#X13sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     total_train \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tot_batch\n","File \u001b[1;32mc:\\Users\\gbert\\anaconda3\\lib\\site-packages\\tqdm\\notebook.py:259\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[1;32m--> 259\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[0;32m    260\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    261\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m    262\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\gbert\\anaconda3\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\gbert\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[1;32mc:\\Users\\gbert\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1358\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1359\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   1360\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1361\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1362\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\gbert\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1325\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1324\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m-> 1325\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   1326\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1327\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n","File \u001b[1;32mc:\\Users\\gbert\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1176\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(failed_workers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1175\u001b[0m     pids_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(w\u001b[39m.\u001b[39mpid) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1176\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mDataLoader worker (pid(s) \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) exited unexpectedly\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(pids_str)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   1177\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, queue\u001b[39m.\u001b[39mEmpty):\n\u001b[0;32m   1178\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n","\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 7636, 17800, 29012, 39816) exited unexpectedly"]}],"source":["import time\n","\n","BATCH_SIZE = 64\n","N_EPOCHS = 100\n","\n","# TODO\n","dataset_size = len(ds)\n","test_split = 0.4\n","test_size = int(test_split * dataset_size)\n","train_size = dataset_size - test_size\n","\n","train_dataset, validation_dataset = torch.utils.data.random_split(ds, [train_size, test_size])\n","train_dataloader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n","validation_dataloader = torch.utils.data.DataLoader(\n","    validation_dataset, batch_size=BATCH_SIZE, num_workers=4)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","if torch.cuda.is_available():\n","    model = model.cuda()\n","    criterion = criterion.cuda()\n","    device = torch.device('cuda:0')\n","    print(f'device: {device}')\n","else:\n","    device = torch.device('cpu')\n","\n","model.to(device) \n","criterion = criterion.to(device)\n","optimizer = optim.Adam(model.parameters(), lr=2.75e-4)\n","\n","\n","start_time = time.time()\n","train_losses = []\n","train_accuracies = []\n","validation_losses = []\n","validation_accuracies = []\n","\n","pbar = tqdm(range(N_EPOCHS))\n","\n","for i in pbar:\n","    total_train_correct = 0.0\n","    total_validation_correct = 0.0\n","    total_train = 0\n","    total_val = 0\n","\n","    model.train()\n","\n","    for batch in tqdm(train_dataloader, leave=False):\n","        tot_batch, count_batch_correct = train(model=model,optimizer=optimizer,criterion=criterion,device=device,batch=batch)\n","        total_train += tot_batch\n","        total_train_correct += count_batch_correct\n","\n","    with torch.no_grad():\n","        for batch in validation_dataloader:\n","            tot_batch, count_batch_correct = eval(model=model,optimizer=optimizer,criterion=criterion,device=device,batch=batch)\n","            total_val += tot_batch\n","            total_validation_correct += count_batch_correct\n","\n","    mean_train_loss = total_train / len(train_dataset)\n","    train_accuracy = total_train_correct / len(train_dataset)\n","\n","    mean_validation_loss = total_val / len(validation_dataset)\n","    validation_accuracy = total_validation_correct / len(validation_dataset)\n","\n","    train_losses.append(mean_train_loss)\n","    validation_losses.append(mean_validation_loss)\n","\n","    train_accuracies.append(train_accuracy)\n","    validation_accuracies.append(validation_accuracy)\n","\n","    pbar.set_postfix({'train_loss': mean_train_loss, 'validation_loss': mean_validation_loss,\n","                     'train_accuracy': train_accuracy, 'validation_accuracy': validation_accuracy})\n","\n","end_time = time.time()\n","duration = end_time - start_time\n","accuracy = test_model(model)\n","assert duration < 600, 'execution took f{duration:.2f} seconds, which longer than 10 mins'\n","assert accuracy > 0.99, f'accuracy is too low, got {accuracy}, need 0.99'\n","print('tests passed')\n"]},{"cell_type":"markdown","metadata":{"id":"sB0EVNBtDhpN"},"source":["## Variable delay model\n","\n","Now, to make this more complicated, we want to have variable delay. So, now, the goal is to transform a sequence of pairs (character, delay) into a character sequence with given delay. Delay is constant within one sequence.\n"]},{"cell_type":"markdown","metadata":{"id":"3i_iwX_AEOCH"},"source":["### Dataset\n","\n","As before, we first implement the dataset:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E4G5b8kuEUEd"},"outputs":[],"source":["class VariableDelayEchoDataset(torch.utils.data.IterableDataset):\n","\n","    def __init__(self, max_delay=8, seq_length=20, size=1000):\n","        self.max_delay = max_delay\n","        self.seq_length = seq_length\n","        self.size = size\n","\n","    def __len__(self):\n","        return self.size\n","\n","    def __iter__(self):\n","        for _ in range(self.size):\n","            seq = torch.tensor([random.choice(range(1, N + 1))\n","                               for i in range(self.seq_length)], dtype=torch.int64)\n","            delay = random.randint(0, self.max_delay)\n","            result = torch.cat(\n","                (torch.zeros(delay), seq[:self.seq_length - delay])).type(torch.int64)\n","            yield seq, delay, result\n"]},{"cell_type":"markdown","metadata":{"id":"oTRVOND3HEJZ"},"source":["### Model\n","\n","And the model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYolFIB8Hg0U"},"outputs":[],"source":["class VariableDelayGRUMemory(torch.nn.Module):\n","\n","    def __init__(self, hidden_size, max_delay):\n","        super().__init__()\n","        # TODO\n","\n","    def forward(self, x, delays):\n","        # inputs:\n","        # x - tensor of shape (batch size, seq length, N + 1)\n","        # delays - tensor of shape (batch size)\n","        # returns:\n","        # logits (scores for softmax) of shape (batch size, seq_length, N + 1)\n","\n","        # TODO\n","        pass\n","\n","    @torch.no_grad()\n","    def test_run(self, s, delay):\n","        # This function accepts one string s containing lowercase characters a-z,\n","        # and a delay - the desired output delay.\n","        # You need to map those characters to one-hot encodings,\n","        # then get the result from your network, and then convert the output\n","        # back to a string of the same length, with 0 mapped to ' ',\n","        # and 1-26 mapped to a-z.\n","\n","        # TODO\n","        pass\n"]},{"cell_type":"markdown","metadata":{"id":"riu3qHWgKjsx"},"source":["### Train\n","\n","As before, you're free to do what you want, as long as training finishes within 10 minutes and accuracy is above 0.99 for delays between 0 and 8.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4FZHojnGO3aw"},"outputs":[],"source":["def test_variable_delay_model(model, seq_length=20):\n","    \"\"\"\n","    This is the test function that runs 100 different strings through your model,\n","    and checks the error rate.\n","    \"\"\"\n","    total = 0\n","    correct = 0\n","    for i in range(500):\n","        s = ''.join([random.choice(string.ascii_lowercase)\n","                    for i in range(seq_length)])\n","        d = random.randint(0, model.max_delay)\n","        result = model.test_run(s, d)\n","        if d > 0:\n","            z = zip(s[:-d], result[d:])\n","        else:\n","            z = zip(s, result)\n","        for c1, c2 in z:\n","            correct += int(c1 == c2)\n","        total += len(s) - d\n","\n","    return correct / total\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\t"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJ18Ef6vKi4s"},"outputs":[],"source":["import time\n","start_time = time.time()\n","\n","MAX_DELAY = 8\n","SEQ_LENGTH = 20\n","\n","# TODO: implement model training here.\n","model = None\n","\n","\n","end_time = time.time()\n","assert end_time - start_time < 600, 'executing took longer than 10 mins'\n","assert test_variable_delay_model(model) > 0.99, 'accuracy is too low'\n","print('tests passed')\n"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"850e86733fd11e39368ad0f4d5057a184573ae93c3821caecd584c5b56b00dd3"}}},"nbformat":4,"nbformat_minor":0}
